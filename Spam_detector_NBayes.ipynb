{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam_Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    " \n",
    "DATA_DIR = '/Users/punchh/Documents/my_work/Spam_Detector/enron'\n",
    "target_names = ['ham', 'spam']\n",
    " \n",
    "def get_data(DATA_DIR):\n",
    "    subfolders = ['enron%d' % i for i in range(1,7)]\n",
    " \n",
    "    data = []\n",
    "    target = []\n",
    "    for subfolder in subfolders:\n",
    "        # spam\n",
    "        spam_files = os.listdir(os.path.join(DATA_DIR, subfolder, 'spam'))\n",
    "        for spam_file in spam_files:\n",
    "            with open(os.path.join(DATA_DIR, subfolder, 'spam', spam_file), encoding=\"latin-1\") as f:\n",
    "                data.append(f.read())\n",
    "                target.append(1)\n",
    " \n",
    "        # ham\n",
    "        ham_files = os.listdir(os.path.join(DATA_DIR, subfolder, 'ham'))\n",
    "        for ham_file in ham_files:\n",
    "            with open(os.path.join(DATA_DIR, subfolder, 'ham', ham_file), encoding=\"latin-1\") as f:\n",
    "                data.append(f.read())\n",
    "                target.append(0)\n",
    " \n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will produce two lists: the data list, where each element is the text of an email, and the target list, \n",
    "# which is simply binary (1 meaning spam and 0 meaning ham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "    def clean(self, s): # function to clean up our string by removing punctuation.\n",
    "        \n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        \"\"\" This uses the 3-argument version of str.maketrans\n",
    "            with arguments (x, y, z) where 'x' and 'y'\n",
    "            must be equal-length strings and characters in 'x'\n",
    "            are replaced by characters in 'y'. 'z'\n",
    "            is a string (string.punctuation here)\n",
    "            where each character in the string is mapped\n",
    "            to None \"\"\"\n",
    "        return s.translate(translator)\n",
    " \n",
    "    def tokenize(self, text): # function to tokenize our string into words.\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    " \n",
    "    def get_word_counts(self, words): # function to count up how many of each word appears in a list of words.\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    " \n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 1)\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 0)\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n)\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n)\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    " \n",
    "        for x, y in zip(X, Y):\n",
    "            \"\"\"zip() to map the similar index of multiple containers \n",
    "                so that they can be used just using as single entity.\"\"\"\n",
    "        \n",
    "            c = 'spam' if y == 1 else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    " \n",
    "                self.word_counts[c][word] += count\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "            \n",
    "                # adding Laplace smoothing\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    " \n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    " \n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    " \n",
    "            if spam_score > ham_score:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sudo-code for the algorithm:\n",
    "    1. Compute log class priors by counting how many messages are spam/ham, \n",
    "       dividing by the total number of messages, and taking the log.\n",
    "    2. For each (document, label) pair, tokenize the document into words.\n",
    "    3. For each word, either add it to the vocabulary for spam/ham, \n",
    "       if it isn’t already there, and update the number of counts. \n",
    "       Also add that word to the global vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve extracted all of the data we need from the training data, \n",
    "we can write another function to actually output the class label for new data. \n",
    "To do this classification, we apply Naive Bayes directly. \n",
    "\n",
    "For example, given a document, we need to iterate each of the words \n",
    "and compute log p(w_i|{Spam}) and sum them all up, \n",
    "and we also compute log p(w_i|{Ham}) and sum them all up. \n",
    "Then we add the log class priors and check to see which score is bigger for that document. \n",
    "Whichever is larger, that is the predicted label!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: remember that the log of 0 is undefined! What if we encounter a word that is in the “spam” vocabulary, but not the “ham” vocabulary? Then p(w_i|{Ham}) will be 0! One way around this is to use `Laplace Smoothing`. We simply add 1 to the numerator, but we also have to add the size of the vocabulary to the denominator to balance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def predict(self, X):\\n    result = []\\n    for x in X:\\n        counts = self.get_word_counts(self.tokenize(x))\\n        spam_score = 0\\n        ham_score = 0\\n        for word, _ in counts.items():\\n            if word not in self.vocab: continue\\n            \\n            # adding Laplace smoothing\\n            log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\\n            log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\\n \\n            spam_score += log_w_given_spam\\n            ham_score += log_w_given_ham\\n \\n        spam_score += self.log_class_priors['spam']\\n        ham_score += self.log_class_priors['ham']\\n \\n        if spam_score > ham_score:\\n            result.append(1)\\n        else:\\n            result.append(0)\\n    return result\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def predict(self, X):\n",
    "    result = []\n",
    "    for x in X:\n",
    "        counts = self.get_word_counts(self.tokenize(x))\n",
    "        spam_score = 0\n",
    "        ham_score = 0\n",
    "        for word, _ in counts.items():\n",
    "            if word not in self.vocab: continue\n",
    "            \n",
    "            # adding Laplace smoothing\n",
    "            log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "            log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    " \n",
    "            spam_score += log_w_given_spam\n",
    "            ham_score += log_w_given_ham\n",
    " \n",
    "        spam_score += self.log_class_priors['spam']\n",
    "        ham_score += self.log_class_priors['ham']\n",
    " \n",
    "        if spam_score > ham_score:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input can be a list of document texts; we return a list of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.000%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We’re reserving the first 100 for the testing set, “train”\n",
    "    our Naive Bayes classifier, then compute the accuracy.\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = get_data(DATA_DIR)\n",
    "    MNB = SpamDetector()\n",
    "    MNB.fit(X[100:], y[100:])\n",
    " \n",
    "    pred = MNB.predict(X[:100])\n",
    "    true = y[:100]\n",
    " \n",
    "    accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "    print(\"{0:.3f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
